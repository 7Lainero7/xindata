import os
import json
import zipfile
import requests

import pandas as pd
from tqdm import tqdm
from kaggle.api.kaggle_api_extended import KaggleApi

from appdir import MODEL_NAME, MODEL_DIR, MODEL_URL


def download_kaggle_csv(dataset: str, file_name: str, download_path: str, kaggle_json_path: str = "kaggle.json"):
    """
    –°–∫–∞—á–∏–≤–∞–µ—Ç .csv —Ñ–∞–π–ª —Å Kaggle.

    :param dataset: str, –Ω–∞–ø—Ä–∏–º–µ—Ä 'shohinurpervezshohan/freelancer-earnings-and-job-trends'
    :param file_name: str, –Ω–∞–ø—Ä–∏–º–µ—Ä 'freelancer_earnings_bd.csv'
    :param download_path: str, –ø—É—Ç—å –¥–ª—è —Å–æ—Ö—Ä–∞–Ω–µ–Ω–∏—è —Ñ–∞–π–ª–∞
    :param kaggle_json_path: str, –ø—É—Ç—å –∫ —Ñ–∞–π–ª—É kaggle.json —Å —Ç–æ–∫–µ–Ω–æ–º
    """
    # –£—Å—Ç–∞–Ω–æ–≤–∏—Ç—å –ø–µ—Ä–µ–º–µ–Ω–Ω—ã–µ –æ–∫—Ä—É–∂–µ–Ω–∏—è –¥–ª—è kaggle API
    os.makedirs(os.path.dirname(download_path), exist_ok=True)
    with open(kaggle_json_path, "r") as f:
        creds = json.load(f)
    os.environ["KAGGLE_USERNAME"] = creds["username"]
    os.environ["KAGGLE_KEY"] = creds["key"]

    api = KaggleApi()
    api.authenticate()
    os.makedirs(download_path, exist_ok=True)
    api.dataset_download_file(dataset, file_name, path=download_path, force=True)
    # –†–∞—Å–ø–∞–∫–æ–≤—ã–≤–∞–µ–º, –µ—Å–ª–∏ —Å–∫–∞—á–∞–Ω –∞—Ä—Ö–∏–≤
    zip_path = os.path.join(download_path, file_name + '.zip')
    if os.path.exists(zip_path):
        with zipfile.ZipFile(zip_path, 'r') as zip_ref:
            zip_ref.extractall(download_path)
        os.remove(zip_path)


def drop_freelancer_id(csv_path: str, output_path: str = None):
    """
    –£–¥–∞–ª—è–µ—Ç —Å—Ç–æ–ª–±–µ—Ü 'Freelancer_ID' –∏–∑ csv-—Ñ–∞–π–ª–∞.

    :param csv_path: –ø—É—Ç—å –∫ –∏—Å—Ö–æ–¥–Ω–æ–º—É csv-—Ñ–∞–π–ª—É
    :param output_path: –ø—É—Ç—å –¥–ª—è —Å–æ—Ö—Ä–∞–Ω–µ–Ω–∏—è —Ä–µ–∑—É–ª—å—Ç–∞—Ç–∞ (–µ—Å–ª–∏ None ‚Äî –ø–µ—Ä–µ–∑–∞–ø–∏—Å—ã–≤–∞–µ—Ç –∏—Å—Ö–æ–¥–Ω—ã–π —Ñ–∞–π–ª)
    """
    df = pd.read_csv(csv_path)
    if 'Freelancer_ID' in df.columns:
        df = df.drop(columns=['Freelancer_ID'])
    if output_path is None:
        output_path = csv_path
    df.to_csv(output_path, index=False)


def download_file(url: str, save_path: str):
    """–°–∫–∞—á–∏–≤–∞–µ—Ç —Ñ–∞–π–ª —Å –ø—Ä–æ–≥—Ä–µ—Å—Å-–±–∞—Ä–æ–º."""
    os.makedirs(os.path.dirname(save_path), exist_ok=True)
    headers = {
        "User-Agent": "Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36"
    }
    response = requests.get(url, headers=headers, stream=True)
    response.raise_for_status()
    total_size = int(response.headers.get('content-length', 0))
    progress = tqdm(
        total=total_size,
        unit='B',
        unit_scale=True,
        desc=f"–°–∫–∞—á–∏–≤–∞–Ω–∏–µ {os.path.basename(save_path)}"
    )
    with open(save_path, 'wb') as f:
        for chunk in response.iter_content(chunk_size=8192):
            if chunk:
                f.write(chunk)
                progress.update(len(chunk))
    progress.close()


def ensure_llama_model():
    """–ü—Ä–æ–≤–µ—Ä—è–µ—Ç –Ω–∞–ª–∏—á–∏–µ –º–æ–¥–µ–ª–∏ –∏ —Å–∫–∞—á–∏–≤–∞–µ—Ç –ø—Ä–∏ –Ω–µ–æ–±—Ö–æ–¥–∏–º–æ—Å—Ç–∏."""
    model_path = os.path.join(MODEL_DIR, MODEL_NAME)
    if not os.path.exists(model_path):
        print("üõ†Ô∏è –ù–∞—á–∏–Ω–∞–µ–º –∑–∞–≥—Ä—É–∑–∫—É –º–æ–¥–µ–ª–∏...")
        try:
            download_file(MODEL_URL, model_path)
            print(f"‚úÖ –ú–æ–¥–µ–ª—å —É—Å–ø–µ—à–Ω–æ —Å–æ—Ö—Ä–∞–Ω–µ–Ω–∞ –≤: {model_path}")
        except Exception as e:
            print(f"‚ùå –û—à–∏–±–∫–∞: {e}")
            print("–ü—Ä–æ–≤–µ—Ä—å—Ç–µ –ø–æ–¥–∫–ª—é—á–µ–Ω–∏–µ –∫ –∏–Ω—Ç–µ—Ä–Ω–µ—Ç—É –∏–ª–∏ URL –º–æ–¥–µ–ª–∏.")
    else:
        print(f"–ú–æ–¥–µ–ª—å —É–∂–µ —Å—É—â–µ—Å—Ç–≤—É–µ—Ç: {model_path}")
    return model_path
